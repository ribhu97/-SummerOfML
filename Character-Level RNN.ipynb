{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 137628 chars, 80 unique\n"
     ]
    }
   ],
   "source": [
    "# Loading training data\n",
    "data = open('kafka.txt', 'r').read()\n",
    "\n",
    "chars = list(set(data)) \n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d chars, %d unique' % (data_size, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'r': 0, ' ': 1, 'E': 2, 'f': 3, 'v': 4, '-': 5, '!': 6, ')': 7, '$': 8, 'K': 9, 'm': 10, 'O': 11, 'o': 12, 'g': 13, 'd': 14, '?': 15, '%': 16, 'M': 17, 'R': 18, ':': 19, 'Q': 20, 'w': 21, 'L': 22, 'C': 23, 'F': 24, 'V': 25, \"'\": 26, 'U': 27, '1': 28, 'D': 29, 'ç': 30, 'T': 31, 'j': 32, ';': 33, 'k': 34, 'a': 35, 'S': 36, 'z': 37, '\"': 38, '5': 39, 'X': 40, 'h': 41, 'l': 42, 'J': 43, '/': 44, '3': 45, '4': 46, '@': 47, 'N': 48, '9': 49, 'e': 50, 'y': 51, 'Y': 52, 'A': 53, '*': 54, ',': 55, '.': 56, 'q': 57, '\\n': 58, 'I': 59, '2': 60, '0': 61, 'p': 62, 'c': 63, 'x': 64, '6': 65, 't': 66, 'n': 67, 'u': 68, 'B': 69, '8': 70, '7': 71, 'b': 72, 's': 73, 'H': 74, 'W': 75, 'P': 76, 'i': 77, '(': 78, 'G': 79}\n",
      "{0: 'r', 1: ' ', 2: 'E', 3: 'f', 4: 'v', 5: '-', 6: '!', 7: ')', 8: '$', 9: 'K', 10: 'm', 11: 'O', 12: 'o', 13: 'g', 14: 'd', 15: '?', 16: '%', 17: 'M', 18: 'R', 19: ':', 20: 'Q', 21: 'w', 22: 'L', 23: 'C', 24: 'F', 25: 'V', 26: \"'\", 27: 'U', 28: '1', 29: 'D', 30: 'ç', 31: 'T', 32: 'j', 33: ';', 34: 'k', 35: 'a', 36: 'S', 37: 'z', 38: '\"', 39: '5', 40: 'X', 41: 'h', 42: 'l', 43: 'J', 44: '/', 45: '3', 46: '4', 47: '@', 48: 'N', 49: '9', 50: 'e', 51: 'y', 52: 'Y', 53: 'A', 54: '*', 55: ',', 56: '.', 57: 'q', 58: '\\n', 59: 'I', 60: '2', 61: '0', 62: 'p', 63: 'c', 64: 'x', 65: '6', 66: 't', 67: 'n', 68: 'u', 69: 'B', 70: '8', 71: '7', 72: 'b', 73: 's', 74: 'H', 75: 'W', 76: 'P', 77: 'i', 78: '(', 79: 'G'}\n"
     ]
    }
   ],
   "source": [
    "# Creating 2 dicts to encode and decode char to int\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars)}\n",
    "ix_to_char = { i:ch for i, ch in enumerate(chars)}\n",
    "print(char_to_ix)\n",
    "print(ix_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Creating char vectors\n",
    "vector_for_char_a = np.zeros((vocab_size, 1))\n",
    "vector_for_char_a[char_to_ix['a']] = 1\n",
    "print(vector_for_char_a.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters of model\n",
    "hidden_size = 100\n",
    "seq_length = 25\n",
    "learning_rate = 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size) * 0.01 #input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size) * 0.01 #hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size) * 0.01 #hidden to output\n",
    "bh = np.zeros((hidden_size, 1))\n",
    "by = np.zeros((vocab_size, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossFun(inputs, targets, hprev):\n",
    "  \"\"\"                                                                                                                                                                                         \n",
    "  inputs,targets are both list of integers.                                                                                                                                                   \n",
    "  hprev is Hx1 array of initial hidden state                                                                                                                                                  \n",
    "  returns the loss, gradients on model parameters, and last hidden state                                                                                                                      \n",
    "  \"\"\"\n",
    "  #store our inputs, hidden states, outputs, and probability values\n",
    "  xs, hs, ys, ps, = {}, {}, {}, {} #Empty dicts\n",
    "    # Each of these are going to be SEQ_LENGTH(Here 25) long dicts i.e. 1 vector per time(seq) step\n",
    "    # xs will store 1 hot encoded input characters for each of 25 time steps (26, 25 times)\n",
    "    # hs will store hidden state outputs for 25 time steps (100, 25 times)) plus a -1 indexed initial state\n",
    "    # to calculate the hidden state at t = 0\n",
    "    # ys will store targets i.e. expected outputs for 25 times (26, 25 times), unnormalized probabs\n",
    "    # ps will take the ys and convert them to normalized probab for chars\n",
    "    # We could have used lists BUT we need an entry with -1 to calc the 0th hidden layer\n",
    "    # -1 as  a list index would wrap around to the final element\n",
    "  #init with previous hidden state\n",
    "  # Using \"=\" would create a reference, this creates a whole separate copy\n",
    "  # We don't want hs[-1] to automatically change if hprev is changed\n",
    "  hs[-1] = np.copy(hprev)\n",
    "  #init loss as 0\n",
    "  loss = 0\n",
    "  # forward pass                                                                                                                                                                              \n",
    "  for t in range(len(inputs)):\n",
    "    xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation (we place a 0 vector as the t-th input)                                                                                                                     \n",
    "    xs[t][inputs[t]] = 1 # Inside that t-th input we use the integer in \"inputs\" list to  set the correct\n",
    "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state                                                                                                            \n",
    "    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars                                                                                                           \n",
    "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars                                                                                                              \n",
    "    loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)                                                                                                                       \n",
    "  # backward pass: compute gradients going backwards    \n",
    "  #initalize vectors for gradient values for each set of weights \n",
    "  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "  dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "  dhnext = np.zeros_like(hs[0])\n",
    "  for t in reversed(range(len(inputs))):\n",
    "    #output probabilities\n",
    "    dy = np.copy(ps[t])\n",
    "    #derive our first gradient\n",
    "    dy[targets[t]] -= 1 # backprop into y  \n",
    "    #compute output gradient -  output times hidden states transpose\n",
    "    #When we apply the transpose weight matrix,  \n",
    "    #we can think intuitively of this as moving the error backward\n",
    "    #through the network, giving us some sort of measure of the error \n",
    "    #at the output of the lth layer. \n",
    "    #output gradient\n",
    "    dWhy += np.dot(dy, hs[t].T)\n",
    "    #derivative of output bias\n",
    "    dby += dy\n",
    "    #backpropagate!\n",
    "    dh = np.dot(Why.T, dy) + dhnext # backprop into h                                                                                                                                         \n",
    "    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity                                                                                                                     \n",
    "    dbh += dhraw #derivative of hidden bias\n",
    "    dWxh += np.dot(dhraw, xs[t].T) #derivative of input to hidden layer weight\n",
    "    dWhh += np.dot(dhraw, hs[t-1].T) #derivative of hidden layer to hidden layer weight\n",
    "    dhnext = np.dot(Whh.T, dhraw) \n",
    "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients                                                                                                                 \n",
    "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " 3MS$gUy%NdO,f\n",
      "RLQ(i:FM\"u20t-%lnb@(cT3ULl2\n",
      "gjAz6k6HY7\"$83$aS8B-kc8\n",
      "qm2ATFpz3@jRsn-çIlq9-0lV\"-dvx6R%\"@wcUKG$aTIGUb:Jw -kwzw4*?i!J/Ia*)Jp\"I/G?reJ!ç/:Px;QryPW$:%wKS9T2m.gNeYQe9U(qk-Mg zçtç*LDwTS'(uzctulf$ \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "#prediction, one full forward pass\n",
    "def forward_pass(h, seed_ix, n):\n",
    "  \"\"\"                                                                                                                                                                                         \n",
    "  sample a sequence of integers from the model                                                                                                                                                \n",
    "  h is memory state, seed_ix is seed letter for first time step   \n",
    "  n is how many characters to predict\n",
    "  \"\"\"\n",
    "  #create vector\n",
    "  x = np.zeros((vocab_size, 1))\n",
    "  #customize it for our seed char\n",
    "  x[seed_ix] = 1\n",
    "  #list to store generated chars\n",
    "  ixes = []\n",
    "  #for as many characters as we want to generate\n",
    "  for t in range(n):\n",
    "    #a hidden state at a given time step is a function \n",
    "    #of the input at the same time step modified by a weight matrix \n",
    "    #added to the hidden state of the previous time step \n",
    "    #multiplied by its own hidden state to hidden state matrix.\n",
    "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "    #compute output (unnormalised)\n",
    "    y = np.dot(Why, h) + by\n",
    "    ## probabilities for next chars\n",
    "    p = np.exp(y) / np.sum(np.exp(y))\n",
    "    #pick one with the highest probability \n",
    "    ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "    #create a vector\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    #customize it for the predicted char\n",
    "    x[ix] = 1\n",
    "    #add it to the list\n",
    "    ixes.append(ix)\n",
    "\n",
    "  txt = ''.join(ix_to_char[ix] for ix in ixes)\n",
    "  print('----\\n %s \\n----' % (txt, ))\n",
    "hprev = np.zeros((hidden_size,1)) # reset RNN memory  \n",
    "#predict the 200 next characters given 'a'\n",
    "forward_pass(hprev,char_to_ix['a'],200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs [11, 67, 50, 1, 10, 12, 0, 67, 77, 67, 13, 55, 1, 21, 41, 50, 67, 1, 79, 0, 50, 13, 12, 0, 1]\n",
      "targets [67, 50, 1, 10, 12, 0, 67, 77, 67, 13, 55, 1, 21, 41, 50, 67, 1, 79, 0, 50, 13, 12, 0, 1, 36]\n"
     ]
    }
   ],
   "source": [
    "# Feeding loss function with inputs and targets\n",
    "p=0  \n",
    "inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "print(\"inputs\", inputs)\n",
    "targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "print(\"targets\", targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 109.550658\n",
      "----\n",
      " s%L)0cGç)oM'o;qY54/dFp9xa6b1ax;zsiS3xm\"''%M$/V d.02\n",
      "pm3t@*W\n",
      "xh,)o:?K'yNs)qf-XxJl1YP?/;zpnXAz-KBFpI$8hqqch09L)N7lrEy8$@c4çç35VN% z8u6re05d*.zCIe9sg\"oJFfoDNNJIud';Q@ulV@NqTGh$bbi.XDTA8JJieafl83)L0?$/qs@ \n",
      "----\n",
      "iter 1000, loss: 82.593661\n",
      "----\n",
      " ias ;ee chichit. -weiut trelf be ce hiof ve w\" we \"ist cann Thd Anf desuss se pe lis bef ey ceekoen cono il Gnd \n",
      "nasd ib thisf yistk ifss iule thace canghen hicd ant lot baplsildware freg this all \"ar \n",
      "----\n",
      "iter 2000, loss: 65.058914\n",
      "----\n",
      " ty. ghenof and whinjeritor wo meavet ane. of iow wosless oouh. her eveleruinshamo mithen a?t mosedon nermle krfore- suaxkes aHy wire was tither wore hat eo seny has sisuliomimouid nra hisperesurheund  \n",
      "----\n",
      "iter 3000, loss: 56.152952\n",
      "----\n",
      " t rasa hisled; ly the fedele tieven, that; her; ttours napmed fur at trre wind iistersurywate ane ansesimor his dither, beas buxtharsaading, wead out; the wooo enedestounte win to wermorolag ans shere \n",
      "----\n",
      "iter 4000, loss: 51.974980\n",
      "----\n",
      "  The rior de lor to cawery away theld wlounted ut that.\n",
      "\n",
      "At the was. Gregor to ay riighing hip aBest watiite wank itiver ther nateven. She otomitwdelin way uly thely at loughtin weatr the mesiyt bivin \n",
      "----\n",
      "iter 5000, loss: 54.154666\n",
      "----\n",
      " er wom. the erert- thithing they ase and the 1os pfichis bay. thaye tt. Ho githinP bark prof (his hiak sipere ghs coreade- the fermoutithay\n",
      "at as eritade its.\n",
      "\n",
      "1\n",
      "\n",
      "1rpencon the E. The morkan, the corpa \n",
      "----\n",
      "iter 6000, loss: 54.691033\n",
      "----\n",
      " ed vo say erinther he comue foun mons t't Bork har if and whave furing of fompate yoming the weat now. And. He hibcen aim. He wouly oly ardy he in and the stibrt of. day to lowcas aven wasked toked wa \n",
      "----\n",
      "iter 7000, loss: 50.542991\n",
      "----\n",
      " eat nom his for pack thieg heac o'su tourd theuct; the mould He domen nor tide his and oven that it ally aln his ald as him he freameed lato in had waq He pfeatowfor at a domleing and diver snon than  \n",
      "----\n",
      "iter 8000, loss: 47.619912\n",
      "----\n",
      " th thine as methe sored or to hiscerd now wask; if this nocle savecse coved his palterte to mesne lotines dis cher acotieter ppeadilfsele coment toon her as all rithese to out nofuisto to overy ta as  \n",
      "----\n",
      "iter 9000, loss: 46.458576\n",
      "----\n",
      " ichare hers, hebmed any aytibifforsid tolevind armemstoovingsithingly about thepedowing futhor thetkeathatly antthest, larleded toounk, tached a chyte, Gungly as has baykish dod and it toor homsersere \n",
      "----\n",
      "iter 10000, loss: 45.979116\n",
      "----\n",
      " to wagk her bect thing\" was comisi Sither the kit lorkef shit ors. Mot not coach tioutd stit. Mnowning rike sthist.\n",
      "\n",
      "Ther itsheack him tound wookly haviny and had litt hif to werkel theted antometioni \n",
      "----\n",
      "iter 11000, loss: 51.628918\n",
      "----\n",
      " \n",
      "me\n",
      "\n",
      "plong the getked.\n",
      "Tase Gnder\n",
      "ch ag,  in tebll/and ulsecedax wejespeptly\n",
      "E:: of renbid is he Found chatibhing, me wertens the coficitay\n",
      " Gugenbergutacuatu, uple ard sy rockunbousisich: Mate aghit  \n",
      "----\n",
      "iter 12000, loss: 48.864182\n",
      "----\n",
      " he spree preverment forte. himd ivl and shiRge! latsdpre was noc sady ofle tle o oprDy tawn und if timulf ca retenf the every derutag. gidst do thos; ther, Mrthe there up at his righ the o thlo projur \n",
      "----\n",
      "iter 13000, loss: 46.051587\n",
      "----\n",
      " oomcimaticame her caing she's oflay had and\n",
      "r. Gpegorished waden the dood room the kack he have are, shasopace any, anperto been the onty sobet not seed the leot al hified him the whove have tather. S \n",
      "----\n",
      "iter 14000, loss: 44.421339\n",
      "----\n",
      " was ind beewige anver tiped to was sonime, wing ap bro\"blano spon and would stave was but clacklo not theay wit himsing of ningit thes fom pane wlught; ic wes bealy sanenwal!\n",
      " Thatjrearin wain him him \n",
      "----\n",
      "iter 15000, loss: 43.976927\n",
      "----\n",
      " it eevour strard, the begard; preutitimether thimremirt in tor's her exthe pacher the  ap but threatully nack If woy tuld and mime to wind clomes. He lommode to eave whenstus lealder taedidistiuts bod \n",
      "----\n",
      "iter 16000, loss: 46.243926\n",
      "----\n",
      " eo torrioed then. He Suvine wot ibatinem whemesaidiiget! Pmot\n",
      "sis disthougto witingcy do lind cepy 1fut copioped, doreing, of the the or andop worksed to his her Gurnight wastend in, of he med cory ti \n",
      "----\n",
      "iter 17000, loss: 50.066464\n",
      "----\n",
      " ist the s.pas tailed tore of has to loon came de bus is to prode alm falpenffod sis but strome in that themd and in tend ly.\n",
      "\" whe prong dang. Buled go was but of trat go knor\n",
      "s for she a ligh was edo \n",
      "----\n",
      "iter 18000, loss: 46.264546\n",
      "----\n",
      " t ik froking aroaply alaileis freay but utoweld tregert all exand the keakelve olligr he had his might if the to his thon the comentes! go mod as father he dong on the fua goy aghecused preeteved user \n",
      "----\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-eea9a7b2dad8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0;31m# forward seq_length characters through the net and fetch gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m   \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWxh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWhh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdby\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlossFun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhprev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m   \u001b[0msmooth_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmooth_loss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.999\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-88bfb1fb3cce>\u001b[0m in \u001b[0;36mlossFun\u001b[0;34m(inputs, targets, hprev)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mdbh\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdhraw\u001b[0m \u001b[0;31m#derivative of hidden bias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mdWxh\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdhraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#derivative of input to hidden layer weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0mdWhh\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdhraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#derivative of hidden layer to hidden layer weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0mdhnext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWhh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdhraw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mdparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdWxh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWhh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdby\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad                                                                                                                \n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0                                                                                                                        \n",
    "while n<=1000*100:\n",
    "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "  # check \"How to feed the loss function to see how this part works\n",
    "  if p+seq_length+1 >= len(data) or n == 0:\n",
    "    hprev = np.zeros((hidden_size,1)) # reset RNN memory                                                                                                                                      \n",
    "    p = 0 # go from start of data                                                                                                                                                             \n",
    "  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "  targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "  # forward seq_length characters through the net and fetch gradient                                                                                                                          \n",
    "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "\n",
    "  # sample from the model now and then                                                                                                                                                        \n",
    "  if n % 1000 == 0:\n",
    "    print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "    forward_pass(hprev, inputs[0], 200)\n",
    "\n",
    "  # perform parameter update with Adagrad                                                                                                                                                     \n",
    "  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by],\n",
    "                                [dWxh, dWhh, dWhy, dbh, dby],\n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "    mem += dparam * dparam\n",
    "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update                                                                                                                   \n",
    "\n",
    "  p += seq_length # move data pointer                                                                                                                                                         \n",
    "  n += 1 # iteration counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
